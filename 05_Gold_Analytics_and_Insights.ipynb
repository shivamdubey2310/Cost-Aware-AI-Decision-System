{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edeff8be-321b-4c04-b602-8ddadc6f57e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 05_Gold_Analytics_and_Insights\n",
    "\n",
    "## Business Value Quantification & ROI Analysis\n",
    "\n",
    "This notebook provides:\n",
    "- **Executive-ready business metrics**\n",
    "- **ROI calculations** comparing our approach to baselines\n",
    "- **What-if scenarios** for capacity planning\n",
    "- **Actionable insights** for stakeholders\n",
    "\n",
    "**Evaluation Alignment**:\n",
    "- Business Impact & Practical Use\n",
    "- AI Innovation & Insight Generation\n",
    "- Documentation & Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2131f3-2d03-4d0a-8a4f-84b13c62ba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup & Table References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bf7649-d58a-452f-a23e-b245cde4ef40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    sum as spark_sum, avg, count, \n",
    "    round as spark_round, col, when, lit\n",
    ")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# Table references\n",
    "GOLD_TABLE = \"cost_aware_capstone.risk_decisioning.gold_decision_recommendations\"\n",
    "SILVER_TABLE = \"cost_aware_capstone.risk_decisioning.silver_cost_aware_features\"\n",
    "RISK_TABLE = \"cost_aware_capstone.risk_decisioning.ml_risk_predictions\"\n",
    "\n",
    "# Business constants\n",
    "DAILY_CAPACITY = 50\n",
    "INVESTIGATOR_HOURLY_RATE = 50  # $ per hour\n",
    "AVG_INVESTIGATION_TIME = 0.75  # hours\n",
    "\n",
    "print(\"Analytics notebook configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84c0ba0-7b7c-4f5d-9ef0-f45ed5054c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 1. ROI Analysis: Cost-Aware vs Baseline Strategies\n",
    "\n",
    "### Quantifying the Value of AI-Driven Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28b290f-8117-4bed-8894-cb9cd0e00d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare multiple strategies\n",
    "strategy_comparison = spark.sql(f\"\"\"\n",
    "    WITH ranked AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (ORDER BY expected_savings_if_investigated DESC) AS cost_aware_rank,\n",
    "            ROW_NUMBER() OVER (ORDER BY risk_probability DESC) AS risk_first_rank,\n",
    "            ROW_NUMBER() OVER (ORDER BY fraud_loss_if_missed DESC) AS loss_first_rank,\n",
    "            ROW_NUMBER() OVER (ORDER BY RAND(42)) AS random_rank\n",
    "        FROM {GOLD_TABLE}\n",
    "    )\n",
    "    SELECT 'Cost-Aware (Ours)' AS strategy,\n",
    "           ROUND(SUM(CASE WHEN cost_aware_rank <= {DAILY_CAPACITY} THEN expected_savings_if_investigated END), 2) AS savings,\n",
    "           ROUND(SUM(CASE WHEN cost_aware_rank <= {DAILY_CAPACITY} THEN investigation_cost END), 2) AS cost\n",
    "    FROM ranked\n",
    "    UNION ALL\n",
    "    SELECT 'Risk-First', \n",
    "           ROUND(SUM(CASE WHEN risk_first_rank <= {DAILY_CAPACITY} THEN expected_savings_if_investigated END), 2),\n",
    "           ROUND(SUM(CASE WHEN risk_first_rank <= {DAILY_CAPACITY} THEN investigation_cost END), 2)\n",
    "    FROM ranked\n",
    "    UNION ALL\n",
    "    SELECT 'Loss-First',\n",
    "           ROUND(SUM(CASE WHEN loss_first_rank <= {DAILY_CAPACITY} THEN expected_savings_if_investigated END), 2),\n",
    "           ROUND(SUM(CASE WHEN loss_first_rank <= {DAILY_CAPACITY} THEN investigation_cost END), 2)\n",
    "    FROM ranked\n",
    "    UNION ALL\n",
    "    SELECT 'Random',\n",
    "           ROUND(SUM(CASE WHEN random_rank <= {DAILY_CAPACITY} THEN expected_savings_if_investigated END), 2),\n",
    "           ROUND(SUM(CASE WHEN random_rank <= {DAILY_CAPACITY} THEN investigation_cost END), 2)\n",
    "    FROM ranked\n",
    "\"\"\")\n",
    "\n",
    "strategy_pdf = strategy_comparison.toPandas()\n",
    "strategy_pdf['roi'] = strategy_pdf['savings'] / strategy_pdf['cost']\n",
    "strategy_pdf['net_value'] = strategy_pdf['savings'] - strategy_pdf['cost']\n",
    "\n",
    "print(\"STRATEGY COMPARISON (50 Investigations)\")\n",
    "print(\"=\" * 70)\n",
    "display(strategy_comparison)\n",
    "\n",
    "# Calculate improvement over baselines\n",
    "our_savings = strategy_pdf[strategy_pdf['strategy'] == 'Cost-Aware (Ours)']['savings'].values[0]\n",
    "risk_savings = strategy_pdf[strategy_pdf['strategy'] == 'Risk-First']['savings'].values[0]\n",
    "random_savings = strategy_pdf[strategy_pdf['strategy'] == 'Random']['savings'].values[0]\n",
    "\n",
    "print(f\"\\nIMPROVEMENT OVER BASELINES:\")\n",
    "print(f\"   vs Risk-First: +{(our_savings - risk_savings)/risk_savings*100:.1f}%\")\n",
    "print(f\"   vs Random:     +{(our_savings - random_savings)/random_savings*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b1f9ea1-3b48-4ac1-94bd-ede7b66b419e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 2. What-If Analysis: Capacity Scenarios\n",
    "\n",
    "### Business Question: What if we hire more investigators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919863b4-7f5c-4d2f-aad9-b5fe857b76bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate different capacity scenarios\n",
    "capacity_scenarios = [25, 50, 75, 100, 150, 200]\n",
    "scenario_results = []\n",
    "\n",
    "for cap in capacity_scenarios:\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            {cap} AS capacity,\n",
    "            ROUND(SUM(CASE WHEN priority_rank <= {cap} THEN expected_savings_if_investigated END), 2) AS total_savings,\n",
    "            ROUND(SUM(CASE WHEN priority_rank <= {cap} THEN investigation_cost END), 2) AS total_cost\n",
    "        FROM {GOLD_TABLE}\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    result['net_value'] = result['total_savings'] - result['total_cost']\n",
    "    result['roi'] = result['total_savings'] / result['total_cost']\n",
    "    result['marginal_savings'] = result['total_savings'] - scenario_results[-1]['total_savings'].values[0] if scenario_results else result['total_savings']\n",
    "    scenario_results.append(result)\n",
    "\n",
    "scenarios_df = pd.concat(scenario_results, ignore_index=True)\n",
    "\n",
    "print(\"CAPACITY SCENARIO ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(scenarios_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(scenarios_df['capacity'], scenarios_df['total_savings'], 'go-', lw=2, markersize=8, label='Expected Savings')\n",
    "axes[0].plot(scenarios_df['capacity'], scenarios_df['total_cost'], 'rs--', lw=2, markersize=8, label='Investigation Cost')\n",
    "axes[0].axvline(x=50, color='blue', linestyle=':', lw=2, alpha=0.7, label='Current Capacity')\n",
    "axes[0].set_xlabel('Investigation Capacity', fontsize=11)\n",
    "axes[0].set_ylabel('Amount ($)', fontsize=11)\n",
    "axes[0].set_title('Savings vs Cost by Capacity', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(scenarios_df['capacity'], scenarios_df['roi'], 'b^-', lw=2, markersize=8)\n",
    "axes[1].axvline(x=50, color='red', linestyle=':', lw=2, alpha=0.7, label='Current Capacity')\n",
    "axes[1].set_xlabel('Investigation Capacity', fontsize=11)\n",
    "axes[1].set_ylabel('ROI (Savings / Cost)', fontsize=11)\n",
    "axes[1].set_title('ROI Diminishing Returns', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINSIGHT: Increasing capacity from 50->75 would increase savings but with lower marginal ROI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc159354-0d83-4ee5-bf1a-0416b749e62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 3. Risk Coverage Analysis\n",
    "\n",
    "### How well do we cover high-risk cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c485940-b20e-44c4-a39d-2115674cf82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Risk coverage by decile\n",
    "risk_coverage = spark.sql(f\"\"\"\n",
    "    WITH deciles AS (\n",
    "        SELECT *,\n",
    "               NTILE(10) OVER (ORDER BY risk_probability) AS risk_decile\n",
    "        FROM {GOLD_TABLE}\n",
    "    )\n",
    "    SELECT\n",
    "        risk_decile,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(decision) AS investigated,\n",
    "        ROUND(SUM(decision) * 100.0 / COUNT(*), 1) AS coverage_pct,\n",
    "        ROUND(AVG(risk_probability) * 100, 1) AS avg_risk_pct,\n",
    "        ROUND(SUM(expected_savings_if_investigated * decision), 2) AS savings_captured\n",
    "    FROM deciles\n",
    "    GROUP BY risk_decile\n",
    "    ORDER BY risk_decile\n",
    "\"\"\")\n",
    "\n",
    "print(\"RISK COVERAGE BY DECILE\")\n",
    "print(\"=\" * 70)\n",
    "display(risk_coverage)\n",
    "\n",
    "coverage_pdf = risk_coverage.toPandas()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(coverage_pdf['risk_decile'], coverage_pdf['coverage_pct'], \n",
    "              color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, 10)))\n",
    "ax.set_xlabel('Risk Decile (1=Lowest, 10=Highest)', fontsize=11)\n",
    "ax.set_ylabel('Investigation Coverage (%)', fontsize=11)\n",
    "ax.set_title('Investigation Coverage by Risk Decile', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, coverage_pdf['coverage_pct']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{val:.0f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINSIGHT: High coverage in high-risk deciles, but we also investigate\")\n",
    "print(\"   some lower-risk cases when their potential loss is high.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7de11f6-48f3-47b5-af84-dd55710a0a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 4. Actionable Insights for Stakeholders\n",
    "\n",
    "### Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ca6b40-99c1-4f59-b346-0dc2f166dac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "KEY FINDING #1: Cost-Aware Outperforms Traditional Approaches - \n",
    "   Our system captures significantly more value than risk-first or\n",
    "   random selection strategies by considering both probability AND\n",
    "   financial impact of each case.\n",
    "\n",
    "   ACTION: Implement cost-aware prioritization in production.\n",
    "\n",
    "\n",
    "KEY FINDING #2: Diminishing Returns Beyond Current Capacity  -  \n",
    "\n",
    "   ROI decreases as we increase investigation capacity, indicating\n",
    "   we're already capturing the highest-value cases.\n",
    "\n",
    "   ACTION: Expanding capacity yields lower marginal returns.\n",
    "             Consider automation for lower-priority cases instead.    \n",
    "\n",
    "\n",
    "KEY FINDING #3: Some Low-Risk Cases Have High Value - \n",
    "   Cases with moderate risk but very high potential losses are\n",
    "   correctly prioritized by our system.\n",
    "\n",
    "   ACTION: Don't filter purely on risk score. Use expected value.\n",
    "\n",
    " \n",
    "KEY FINDING #4: Investigation Costs Are Well-Justified     - \n",
    "   ROI > 1 indicates investigations generate positive net value.\n",
    "   Every dollar spent on investigation returns multiple dollars\n",
    "   in prevented fraud losses.\n",
    "\n",
    "   ACTION: Maintain or increase investigation budget - it's profitable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5031e4d5-e1ff-44ea-bed0-bbb312d36ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Notebook Complete\n",
    "\n",
    "**Next Steps**:\n",
    "1. Review `06_Interactive_Dashboard.ipynb` for detailed visualizations\n",
    "2. Present findings to stakeholders\n",
    "3. Implement in production environment\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Gold_Analytics_and_Insights",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
